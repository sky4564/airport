# 검색엔진 최적화 기초

## rule1 검색엔진 최적화의 목적

> 출처: [네이버 검색 어드바이저 SEO 기초 가이드](https://searchadvisor.naver.com/guide/seo-basic-intro)

### 필수 설정 작업

#### 웹마스터도구 등록

- **등록 단위**: 호스트 단위만 지원
  - ✅ `http://www.mysite.com`
  - ✅ `http://blog.mysite.com`
  - ❌ `http://www.mysite.com/myid`
- **소유확인 방법**: HTML meta 태그 또는 HTML 파일 업로드

#### 소유확인 실패시 체크사항

- [ ] 방화벽에서 네이버 검색로봇(Yeti) 차단 여부
- [ ] 메인페이지 redirect 방식 (HTTP redirect 사용 권장)
- [ ] frame 태그 내부 소유확인 태그 배치 여부
- [ ] 업로드 HTML 파일 손상 여부

#### 네이버 검색로봇 접근 허용

```txt
# robots.txt 설정
User-agent: Yeti
Allow: /

# 또는 모든 검색엔진
User-agent: *
Allow: /
```

**검색로봇 정보**:

- **이름**: Yeti
- **User-Agent**: `Mozilla/5.0 (compatible; Yeti/1.1; +https://naver.me/spd)`
- **중요**: IP 기반 차단 금지 (IP 대역 변경 가능)

---

### 콘텐츠 최적화

#### 중복 콘텐츠 방지

- ❌ 동일 콘텐츠로 여러 사이트 개설
- ❌ 모든 페이지 동일 제목
- ✅ 페이지별 고유 제목 및 콘텐츠

#### 사이트맵 & RSS 제출

- **사이트맵**: XML 형식, 수집 대상 URL 목록
- **RSS**: 최신글 본문 전체 포함
- **제출 위치**: 웹마스터도구 "요청" 메뉴

#### 연관 채널 설정

- 네이버 블로그, 카페, 페이스북, 인스타그램 등
- 채널 정보 마크업 추가

---

### 검증 및 문제해결

#### 검색 결과 확인

- **반영 시간**: 검색로봇 방문 후 최대 1주일
- **확인 방법**:
  - 네이버 검색창에 사이트명 입력
  - `site:www.mysite.com` 질의 사용
- **성과 확인**: 웹마스터도구 노출/클릭 리포트

#### 메인페이지 검색 노출 안될 때

```html
<!-- 제거해야 할 태그 -->
<meta name="robots" content="noindex" />
```

- [ ] robots.txt 및 방화벽 설정 재확인
- [ ] noindex 메타태그 제거
- [ ] HTML frame 태그 사용 여부 확인
- [ ] 제목/설명 정책 위반 여부 확인

#### 콘텐츠 검색 반영 안될 때

- [ ] noindex 메타태그 제거
- [ ] JavaScript 기반 콘텐츠 로딩 확인
- [ ] HTML frame 태그 제거
- [ ] JavaScript redirect → HTTP redirect 변경

#### 사이트 내 문서 수 적을 때

```html
<!-- 제거해야 할 태그 -->
<meta name="robots" content="nofollow" />
```

- [ ] 사이트맵/RSS 링크 상대경로 → 절대경로
- [ ] nofollow 메타태그 제거
- [ ] JavaScript 링크 → 표준 HTML 링크

---

### AI 작업 체크리스트

#### 기술적 설정

- [ ] robots.txt에 Yeti 허용 설정
- [ ] 네이버 웹마스터도구 등록
- [ ] 사이트맵 XML 생성 및 제출
- [ ] RSS 피드 설정 (전체 본문 포함)

#### HTML 구조 최적화

```html
<!-- 권장 링크 구조 -->
<a href="https://www.mysite.com/page">Link</a>

<!-- 피해야 할 구조 -->
<span onClick="javascript:goto(A)">Link</span>
```

#### 메타태그 검증

```html
<!-- 제거해야 할 태그들 -->
<meta name="robots" content="noindex" />
<meta name="robots" content="nofollow" />

<!-- frame 태그 사용 금지 -->
<frame src="..."> <!-- 사용 금지 --></frame>
```

#### redirect 방식

- ✅ HTTP redirect (301, 302)
- ✅ HTML meta refresh
- ❌ JavaScript redirect

---

### 절대 금지사항

9. **JavaScript 기반 콘텐츠 로딩** (표준 HTML 권장)
10. **HTML frame 태그 사용** (중요 콘텐츠)
11. **noindex/nofollow 메타태그** (의도적 제외시에만)
12. **JavaScript만으로 링크 처리**
13. **동일 콘텐츠 여러 사이트 개설**
14. **IP 기반 검색로봇 차단**

---

### 네이버 특화 검증 방법

1. **웹마스터도구 등록 및 소유확인**
2. **robots.txt에 Yeti 허용 여부**
3. **사이트맵/RSS 제출 상태**
4. **site:도메인명 검색 결과 확인**
5. **노출/클릭 리포트 모니터링**
6. **JavaScript/frame 태그 사용 여부**
7. **링크 구조 표준 HTML 준수**

모든 웹사이트 작업시 이 10단계 가이드를 순차적으로 적용하여 네이버 검색 최적화를 완성할 것.

---

## rule2 웹 사이트를 만들 때

> 출처: [네이버 검색 어드바이저 웹사이트 생성 가이드](https://searchadvisor.naver.com/guide/seo-basic-create)

### 웹사이트 구조 최적화

#### URL 구조 규칙

##### 단일 호스트명 사용

- **원칙**: 같은 내용에 대해 단일 URL 사용
- **중복 방지**: 여러 hostname 사용시 중복 판정
- **트래픽 최적화**: 불필요한 인터넷 트래픽 방지

##### 301 리다이렉트 설정

- **대표 주소**: 사람들에게 알리고자 하는 고유 주소
- **호스팅 독립**: 플랫폼에 상관없이 바뀌지 않는 주소
- **www 처리**: 선호도에 따라 한쪽으로 리다이렉트

```typescript
// Next.js 리다이렉트 설정
async redirects() {
  return [
    {
      source: '/:path*',
      has: [{ type: 'host', value: 'www.example.com' }],
      destination: 'https://example.com/:path*',
      permanent: true, // 301 redirect
    }
  ];
}
```

##### Canonical URL 설정

```html
<!-- 대표 주소 명시 -->
<link rel="canonical" href="https://example.com/page" />
```

#### 호스트명 표준 준수

##### 허용 문자

- **알파벳**: a-z, A-Z
- **숫자**: 0-9
- **하이픈**: - (맨 앞/뒤 제외)

##### 금지 문자

- **Underscore(\_)**: RFC 표준 위반
- **한글 도메인**: underscore 사용 불가 (RFC-5890~5893)

##### 표준 준수 이유

- **브라우저 호환성**: 일부 브라우저에서만 작동
- **SNS 링크**: 잘못된 링크 활성화 위험
- **검색엔진**: 표준이 아닌 도메인 인식 문제

---

### 검색로봇 최적화

#### robots.txt 설정

##### 기본 설정

```txt
# 네이버 검색로봇만 허용
User-agent: Yeti
Allow: /

# 모든 검색엔진 허용
User-agent: *
Allow: /

# 사이트맵 위치 명시
Sitemap: https://example.com/sitemap.xml
```

##### 특정 디렉토리 제외

```txt
# 관리자/개인정보 페이지 수집 금지
User-agent: Yeti
Disallow: /private*/
Disallow: /admin/
Disallow: /api/

# 모든 검색로봇에게 수집 금지
User-agent: *
Disallow: /
```

##### 중요 사항

- **파일 위치**: 반드시 루트에 위치 (`https://example.com/robots.txt`)
- **기본 동작**: robots.txt 없으면 모든 페이지 수집 대상
- **권장사항**: 수집 금지 페이지만 Disallow 설정

#### 사이트맵 XML 생성

##### 사이트맵 정의

- **목적**: 수집 대상 URL 목록 제공
- **형식**: XML 형식
- **효과**: 검색로봇 수집 효율성 향상

##### XML 구조

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://example.com/page.html</loc>
    <lastmod>2024-01-01T00:00:00+09:00</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
</urlset>
```

##### 태그 설명

- **`<loc>`**: 페이지 URL (필수)
- **`<lastmod>`**: 마지막 수정일 (선택)
- **`<changefreq>`**: 변경 빈도 (선택)
- **`<priority>`**: 우선순위 0.0~1.0 (선택)

---

### 오류 페이지 처리

#### HTTP 상태 코드 준수

##### 필수 상태 코드

```
404 Not Found: 페이지 없음, URL 잘못 입력
403 Forbidden: 접근 권한 없음, 차단됨
503 Service Unavailable: 서버 과부하, 일시적 서비스 중단
```

##### 올바른 처리 방법

- **HTTP 응답**: 상태 코드와 함께 응답
- **사용자 친화적**: 맞춤형 오류 페이지 제공
- **소프트웨어 호환**: HTTP 규약 준수하는 S/W가 이해 가능

#### 잘못된 처리 방법

- **200 OK 반환**: 오류 내용만 출력하고 200 상태 코드
- **리다이렉트 처리**: 오류 페이지로 리다이렉트
- **빈 페이지**: 내용 없이 상태 코드만 반환

#### 맞춤형 오류 페이지

```html
<!-- 404 페이지 예시 -->
<!DOCTYPE html>
<html>
  <head>
    <title>페이지를 찾을 수 없습니다 - 사이트명</title>
    <meta name="robots" content="noindex" />
  </head>
  <body>
    <h1>404 - 페이지를 찾을 수 없습니다</h1>
    <p>요청하신 페이지가 존재하지 않습니다.</p>
    <a href="/">홈으로 돌아가기</a>
  </body>
</html>
```

---

### 모바일 최적화

#### 반응형 웹사이트 (권장)

##### 정의

- **기법**: 브라우저가 화면 크기에 맞게 자동 조절
- **장점**: 동일 URL로 기기별 최적화 콘텐츠 제공
- **네이버 권장**: 반응형 웹사이트 선호

##### 구현 방법

```html
<!-- viewport 설정 -->
<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- CSS Media Query -->
<style>
  @media (max-width: 768px) {
    .container {
      width: 100%;
    }
  }
</style>
```

#### 별도 모바일 URL

##### 자동 리다이렉트

- **User-Agent 인식**: 모바일 브라우저 자동 감지
- **자동 전환**: 데스크톱 접근시 모바일 사이트로 리다이렉트
- **1:1 대응**: 데스크톱과 모바일 URL 명시적 지정

##### 구현 예시

```javascript
// User-Agent 기반 모바일 감지
const isMobile =
  /Android|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(
    navigator.userAgent
  );

if (isMobile && !window.location.hostname.includes("m.")) {
  window.location.href = window.location.href.replace("www.", "m.");
}
```

---

### AI 작업 체크리스트

#### URL 구조 최적화

- [ ] 단일 호스트명 사용 확인
- [ ] 301 리다이렉트 설정 (www 처리)
- [ ] Canonical URL 설정
- [ ] 호스트명 표준 준수 (underscore 금지)

#### 검색로봇 설정

- [ ] robots.txt 루트 위치 확인
- [ ] Yeti 로봇 허용 설정
- [ ] 사이트맵 XML 생성 및 제출
- [ ] 관리자 페이지 수집 금지 설정

### 오류 페이지 처리

- [ ] HTTP 상태 코드 정상 반환
- [ ] 맞춤형 오류 페이지 제공
- [ ] noindex 메타태그 설정 (오류 페이지)
- [ ] 사용자 친화적 메시지

### 모바일 최적화

- [ ] 반응형 웹 구현 (권장)
- [ ] viewport 메타태그 설정
- [ ] CSS Media Query 적용
- [ ] 별도 모바일 URL시 자동 리다이렉트

### 기술적 검증

- [ ] robots.txt 접근 가능 (`https://example.com/robots.txt`)
- [ ] 사이트맵 XML 유효성 검증
- [ ] HTTP 상태 코드 정상 응답
- [ ] 모바일 반응형 동작 확인

---

### 절대 금지사항

#### URL 구조 관련

1. **동일 콘텐츠 여러 hostname 사용**
2. **JavaScript 리다이렉트 사용** (HTTP 리다이렉트 권장)
3. **Underscore(\_) 포함 호스트명**
4. **Canonical URL 미설정**

#### 검색로봇 관련

5. **robots.txt 루트 위치 이외 배치**
6. **모든 페이지 수집 금지 설정**
7. **사이트맵 XML 미생성**

#### 오류 페이지 관련

8. **오류 내용만 출력하고 200 OK 반환**
9. **오류 페이지로 리다이렉트 처리**
10. **빈 오류 페이지 제공**

#### 모바일 관련

11. **반응형 웹 미구현시 별도 URL 미제공**
12. **User-Agent 기반 리다이렉트 미구현**

---

### 검증 방법

#### 기술적 검증

1. **robots.txt 접근 테스트**: `curl https://example.com/robots.txt`
2. **사이트맵 XML 검증**: XML 유효성 검사
3. **HTTP 상태 코드 확인**: 오류 페이지 테스트
4. **모바일 반응형 테스트**: 다양한 기기에서 확인

### 네이버 검색 확인

1. **site: 질의**: `site:example.com` 검색 결과 확인
2. **웹마스터도구**: 색인 상태 및 오류 확인
3. **모바일 검색**: 모바일에서 사이트 접근 테스트

이 웹사이트 생성 가이드를 모든 새 웹사이트 작업에 체계적으로 적용하여 네이버 검색 최적화를 완성할 것.

---

## rule3 robots.txt 설정하기

> 출처: [네이버 검색 어드바이저 robots.txt 설정 가이드](https://searchadvisor.naver.com/guide/seo-basic-robots)

### robots.txt 기본 개념

#### 정의 및 목적

- **국제 권고안**: IETF 2022년 9월 표준화 문서 발행
- **검색로봇 제어**: 사이트 및 웹페이지 수집 허용/제한
- **네이버 준수**: 네이버 검색로봇은 robots.txt 규칙 준수
- **기본 동작**: robots.txt 없으면 모든 콘텐츠 수집 허용

#### 중요 사항

- **개인정보 보호**: robots.txt만으로는 부족, 로그인 등 추가 보호 필요
- **특수 로봇**: 광고주 정보 취득, 링크 미리보기 등은 규칙 참조 안할 수 있음
- **외부 노출 방지**: 민감한 콘텐츠는 다른 차단 방법 병행 사용

### robots.txt 위치 및 접근

#### 파일 위치

- **루트 디렉터리**: 반드시 사이트 루트에 위치
- **접근 가능**: `http://www.example.com/robots.txt`
- **파일 형식**: 일반 텍스트 파일 (text/plain)

#### HTTP 응답코드별 처리

##### 2xx Successful

- **정상 처리**: 로봇 배제 표준 준수하는 규칙 해석
- **주의사항**: HTML 문서로 반환시 유효한 규칙이 있어도 무시될 수 있음
- **권장사항**: 일반 텍스트 파일로 작성

##### 3xx Redirection

- **허용 횟수**: HTTP redirect 5회까지 허용
- **초과시**: 중단 후 '모두 허용'으로 해석
- **제한사항**: HTML 및 JavaScript redirect는 해석 안함

##### 4xx Client Error

- **해석**: '모두 허용'으로 해석

##### 5xx Server Error

- **해석**: '모두 허용하지 않음'으로 해석
- **예외**: 이전 정상 규칙이 있으면 일시적으로 사용 가능

### robots.txt 규칙 예제

#### 네이버 전용 설정

```txt
# 다른 검색엔진 차단, 네이버만 허용
User-agent: *
Disallow: /
User-agent: Yeti
Allow: /
```

#### 모든 검색엔진 허용

```txt
# 모든 검색엔진 허용
User-agent: *
Allow: /
```

#### 루트 페이지만 허용

```txt
# 루트 페이지만 수집 허용
User-agent: *
Disallow: /
Allow: /$
```

#### 특정 디렉토리 제외

```txt
# 관리자/개인정보 페이지 수집 금지
User-agent: Yeti
Disallow: /private*/
```

#### 전체 차단 (권장하지 않음)

```txt
# 모든 페이지 수집 금지 (권장하지 않음)
User-agent: *
Disallow: /
```

### 특별 주의사항

#### 파비콘(favicon) 수집 허용

- **중요성**: 검색로봇이 웹페이지 콘텐츠의 일부로 판단
- **설정 방법**: 문서와 동일한 규칙 적용 또는 기본 허용
- **참고**: 파비콘 마크업 가이드 참조

#### JavaScript/CSS 파일 경로 확인

- **수집 대상**: HTML뿐만 아니라 JS, CSS 등 리소스도 수집
- **설정 방법**: 참조하는 웹 문서와 동일한 규칙 적용
- **이유**: 현대적 검색로봇이 웹 문서의 한 부분으로 간주

#### sitemap.xml 지정

```txt
User-agent: *
Allow: /
Sitemap: http://www.example.com/sitemap.xml
```

### 웹마스터도구 활용

#### robots.txt 수집 및 검증

- **수집 요청**: 수정 후 빠른 알림을 위한 수집 요청
- **테스트 기능**: 설정된 로봇룰에 따른 수집 가능여부 테스트

#### robots.txt 간단 생성

- **생성 기능**: 간단한 robots.txt 파일 생성 및 다운로드
- **적용 방법**: 다운로드한 파일을 루트 디렉터리에 업로드
- **인식**: 수집 요청 실행으로 네이버 검색로봇 즉시 인식

### AI 작업 체크리스트

#### 기본 설정

- [ ] robots.txt 루트 디렉터리 위치 확인
- [ ] 일반 텍스트 파일 형식 확인
- [ ] HTTP 2xx 응답코드 정상 반환 확인

#### 규칙 설정

- [ ] User-agent 설정 (Yeti 또는 \*)
- [ ] Allow/Disallow 규칙 적절히 설정
- [ ] 특정 디렉토리 제외 설정 (관리자 페이지 등)
- [ ] sitemap.xml 위치 명시

#### 특별 파일 처리

- [ ] 파비콘 수집 허용 설정
- [ ] JavaScript/CSS 파일 경로 규칙 확인
- [ ] 리소스 파일 접근 가능 확인

#### 검증 및 테스트

- [ ] 웹마스터도구 robots.txt 도구 활용
- [ ] 수집 요청 실행
- [ ] 테스트 기능으로 규칙 검증
- [ ] 실제 로봇 접근 테스트

### 절대 금지사항

#### 파일 위치 관련

1. **루트 디렉터리 이외 위치**
2. **HTML 형식으로 반환**
3. **접근 불가능한 파일**

#### 규칙 설정 관련

4. **전체 사이트 수집 금지** (특별한 경우 제외)
5. **파비콘 경로 차단**
6. **필수 리소스 파일 차단**

#### 기술적 문제

7. **HTTP 5xx 에러 반환**
8. **무한 리다이렉트 발생**
9. **파일 손상 또는 인코딩 오류**

### 검증 방법

#### 기술적 검증

1. **파일 접근 테스트**: `curl https://example.com/robots.txt`
2. **HTTP 상태코드 확인**: 2xx 응답 확인
3. **파일 형식 검증**: text/plain MIME 타입 확인
4. **규칙 문법 검증**: 올바른 robots.txt 문법 확인

#### 웹마스터도구 활용

1. **robots.txt 수집 요청**: 수정 후 즉시 반영
2. **테스트 기능**: 설정된 규칙에 따른 수집 가능여부 확인
3. **간단 생성 도구**: 기본 템플릿 활용

#### 실제 테스트

1. **다양한 User-agent 테스트**: Yeti, Googlebot 등
2. **특정 URL 접근 테스트**: Allow/Disallow 규칙 확인
3. **리소스 파일 접근 테스트**: JS, CSS, 이미지 등

이 robots.txt 설정 가이드를 모든 웹사이트 작업에 체계적으로 적용하여 네이버 검색 최적화를 완성할 것.

---

## rule4 웹 페이지의 이동

> 출처: [네이버 검색 어드바이저 웹페이지 이동 가이드](https://searchadvisor.naver.com/guide/seo-basic-redirect)

### JavaScript 리다이렉트 금지

#### 금지 이유

- **검색로봇 인식 어려움**: JavaScript 형식은 검색로봇이 이해하기 어려움
- **스팸성 문서 판정**: 리다이렉트 정보를 숨기는 형태로 스팸 문서로 분류될 위험
- **중요도 저평가**: 양질의 페이지임에도 불필요하게 중요도가 낮게 평가될 수 있음

#### 잘못된 사용 예시

```javascript
// 금지: JavaScript 리다이렉트
var timerId = setTimeout("move()", 500);
function move() {
  location.href = "http://new.site.com/";
  timerId = 0;
}
```

#### 대안 방법

##### Meta Refresh (차선책)

```html
<!-- 메타 리프레시 사용 -->
<meta http-equiv="refresh" content="10;url=http://new.site.com" />
```

- **네이버 인식**: 네이버 검색로봇이 인식함
- **HTML 4.01 표준**: 권장하지 않음
- **권장사항**: HTTP 리다이렉트 사용 권장

##### HTTP 리다이렉트 (권장)

```typescript
// Next.js 301 리다이렉트
async redirects() {
  return [
    {
      source: '/old-page',
      destination: '/new-page',
      permanent: true, // 301 리다이렉트
    }
  ];
}
```

### HTTP 리다이렉트 종류

#### 301 Permanently Moved (영구 이동)

##### 사용 시기

- **사이트 전체 이전**: 도메인 변경
- **페이지 URL 변경**: 내부 페이지 주소 변경
- **검색 정보 유지**: 기존 검색 정보 및 선호도 정보 활용

##### 중요성

- **검색로봇 인식**: 기존 URL과 신규 URL의 연관성 인식
- **중복 정보 방지**: 기존 정보를 잘못된 신규 정보로 판단 방지
- **검색 노출 도움**: 기존 정보를 활용하여 검색 노출에 큰 도움

##### Apache 웹서버 설정 예시

```apache
# Old host 전체를 New host의 top page로 리다이렉트
Redirect 301 / http://new.site.com
# 또는
Redirect permanent http://new.site.com

# example.com/* 을 www.example.com/* 으로 리다이렉트
RewriteEngine On
RewriteCond %{HTTP_HOST} example.com
RewriteRule ^([^/]*)$ http://www.example.com [R=301,L]
```

#### 302 Temporarily Moved (임시 이동)

##### 사용 시기

- **사이트 장애**: 일시적인 서비스 중단
- **페이지 임시 변경**: 잠시 다른 페이지로 변경
- **기존 정보 유지**: 원래 페이지 정보 삭제 방지

##### 중요성

- **삭제 방지**: 검색로봇이 페이지 삭제로 잘못 판단하는 것 방지
- **정보 유지**: 기존에 수집된 원래 페이지 정보 유지
- **일시적 변경 인지**: 다른 정보로 변경된 것으로 판단하는 것 방지

##### Apache 웹서버 설정 예시

```apache
# 공사중인 페이지로 리다이렉트
Redirect / /under_const.html
# 또는
Redirect temp / /under_const.html
```

### 리다이렉트 주의사항

#### 연속 리다이렉트 횟수 제한

##### 권장 사항

- **최대 5회**: 연속 리다이렉트 횟수는 5회 이하 권장
- **무한 루프 방지**: 자기 자신을 반복해서 리다이렉트하는 경우 수집 안됨
- **최소화 권장**: 가능한 연속된 리다이렉트 사용 최소화

##### 문제 해결 방법

- **개발자 도구 활용**: 브라우저 개발자 도구로 리다이렉트 체인 확인
- **기술담당자 협력**: 사이트 기술담당자와 함께 문제 해결
- **수동 확인 한계**: 브라우저에서 눈으로 확인하기에는 한계점 존재

#### 최종 랜딩 URL 검증

##### 필수 확인 사항

- **존재 여부**: 리다이렉트되는 최종 URL이 실제 존재하는지 확인
- **접근 가능**: 검색로봇이 접근할 수 있는지 확인
- **robots.txt 설정**: 중간에 수집이 불가능한 URL이 섞여있는지 확인

##### 문제 발생 시나리오

- **연결 과정**: 여러 번 연결된 리다이렉트의 모든 주소가 수집 가능해야 함
- **robots.txt 착오**: 중간에 수집이 불가능한 URL이 있으면 전체 수집 실패
- **최종 URL만 가능**: 처음과 최종 URL이 수집 가능해도 중간 과정에서 실패 가능

### 트래픽 제한 방법

#### 웹마스터도구 트래픽 제한 (권장)

##### 사용 시기

- **사이트 성능 문제**: 일시적으로 성능 문제가 있을 때
- **내부 정책 문제**: 잠시 동안 네이버 검색로봇 접근 제한이 필요할 때

##### 설정 방법

- **수집 설정**: 웹마스터도구의 수집 설정에서 트래픽 제한량 최소 지정
- **권장사항**: robots.txt로 차단하는 것보다 권장

##### robots.txt 차단의 문제점

- **문서 제외**: 해당 기간 동안 사이트 내 문서가 네이버 검색에서 제외될 수 있음
- **영향 범위**: 전체 사이트에 영향을 미칠 수 있음

### 리다이렉트 변경 반영 시간

#### 검색 결과 반영 과정

##### 처리 단계

1. **개별 문서 수집**: 검색로봇이 개별 문서를 수집
2. **정보 종합**: 별도 과정을 통해 모아진 정보를 종합
3. **리다이렉트 분석**: 리다이렉트나 사이트 구조를 분석하고 업데이트
4. **정보 갱신**: 이 과정에서 정보의 갱신에 시간이 걸릴 수 있음

##### 권장 사항

- **기존 URL 유지**: 사이트 개편 시 한동안 기존 검색에 노출되던 URL들에도 리다이렉트 설정
- **서비스 연속성**: 검색 엔진의 업데이트에 관계없이 서비스 제공 가능하도록 설정
- **점진적 전환**: 갑작스러운 변경보다는 점진적인 전환 권장

### AI 작업 체크리스트

#### JavaScript 리다이렉트 제거

- [ ] JavaScript location.href 사용 제거
- [ ] setTimeout 기반 리다이렉트 제거
- [ ] Meta refresh로 임시 대체 (HTTP 리다이렉트로 최종 변경)

#### HTTP 리다이렉트 설정

- [ ] 301 리다이렉트: 영구 이동 (사이트/페이지 변경)
- [ ] 302 리다이렉트: 임시 이동 (장애/임시 변경)
- [ ] Next.js redirects() 설정 또는 서버 설정

#### 리다이렉트 체인 검증

- [ ] 연속 리다이렉트 5회 이하 확인
- [ ] 무한 루프 방지 확인
- [ ] 최종 랜딩 URL 존재 및 접근 가능 확인
- [ ] 중간 과정의 모든 URL 수집 가능 확인

#### 트래픽 제한 설정

- [ ] 웹마스터도구 트래픽 제한 사용 (robots.txt 차단 대신)
- [ ] 일시적 제한 시 적절한 방법 선택

#### 검색 결과 반영 대응

- [ ] 기존 URL 리다이렉트 설정 유지
- [ ] 점진적 전환으로 서비스 연속성 보장
- [ ] 개발자 도구로 리다이렉트 체인 확인

### 절대 금지사항

#### JavaScript 관련

1. **JavaScript location.href 사용**
2. **setTimeout 기반 리다이렉트**
3. **검색로봇이 이해하기 어려운 JavaScript 리다이렉트**

#### 리다이렉트 체인 관련

4. **연속 리다이렉트 5회 초과**
5. **자기 자신을 무한 반복하는 리다이렉트**
6. **존재하지 않는 최종 랜딩 URL**
7. **중간 과정에 수집 불가능한 URL 포함**

#### 트래픽 제한 관련

8. **robots.txt로 검색로봇 차단** (웹마스터도구 트래픽 제한 권장)
9. **일시적 제한 시 부적절한 방법 사용**

### 검증 방법

#### 기술적 검증

1. **브라우저 개발자 도구**: Network 탭에서 리다이렉트 체인 확인
2. **curl 명령어**: `curl -I https://example.com/old-page` 로 상태 코드 확인
3. **리다이렉트 체인 테스트**: 각 단계별 접근 가능 여부 확인
4. **최종 URL 접근 테스트**: 최종 랜딩 URL 정상 접근 확인

#### 네이버 검색 확인

1. **site: 질의**: `site:example.com` 검색 결과 확인
2. **웹마스터도구**: 색인 상태 및 오류 확인
3. **검색 결과 반영 시간**: 최대 1주일 소요 가능성 고려

#### 리다이렉트 코드 템플릿

```typescript
// Next.js 리다이렉트 설정
async redirects() {
  return [
    // 301 영구 이동 (사이트 변경)
    {
      source: '/old-page',
      destination: '/new-page',
      permanent: true,
    },
    // 302 임시 이동 (장애/임시 변경)
    {
      source: '/maintenance',
      destination: '/under-construction',
      permanent: false,
    },
    // 도메인 변경
    {
      source: '/:path*',
      has: [{ type: 'host', value: 'old-domain.com' }],
      destination: 'https://new-domain.com/:path*',
      permanent: true,
    }
  ];
}
```

이 리다이렉트 가이드를 모든 웹사이트 작업에 체계적으로 적용하여 네이버 검색 최적화를 완성할 것.

---

## rule5 웹사이트 이전

> 출처: [네이버 검색 어드바이저 웹사이트 이전 가이드](https://searchadvisor.naver.com/guide/seo-basic-migration)

### 사이트 이전 고려사항

#### 콘텐츠 규모별 이전 전략

- **중소규모 사이트**: 모든 URL을 동시에 이동 권장
- **대규모 사이트**: 특정 카테고리별로 단계적 이동 고려
- **도메인 변경**: 반드시 HTTP redirect 설정 필수

#### 품질 지수 전파 시간

- **평가 전파**: A 사이트 품질 지수가 B 사이트로 전파되기까지 시간 소요
- **URL 인식**: 네이버 검색로봇이 변경된 URL을 모두 인식하고 품질 평가 필요
- **브랜드 변경**: 꼭 필요한 경우에만 사이트 이전 고려

### 사이트 이전 절차

#### 1단계: 콘텐츠 이동

- **전체 콘텐츠 이동**: 신규 사이트로 구 사이트 콘텐츠 이동
- **이미지/파일 확인**: 포함된 이미지, 파일 등도 신규 사이트로 이동 확인

#### 2단계: 도메인 할당

- **도메인 설정**: 신규 사이트에 도메인 할당
- **인식 시간**: 검색로봇 인식에 최소 2-3일 소요
- **DNS 설정**: 도메인 제공 업체의 DNS 설정 확인
- **사이트 체크**: 웹마스터도구 사이트 간단 체크에서 접속 여부 확인

#### 3단계: 리다이렉트 설정

- **robots.txt 허용**: 신규 사이트의 robots.txt를 모두 허용
- **301 HTTP redirect**: 구 사이트 URL에서 신규 사이트로 301 리다이렉트 설정
- **1:1 대응**: 콘텐츠 리다이렉트는 URL 기준으로 1:1 대응
- **canonical URL**: 신규 사이트 모든 페이지에 선호 URL 지정

#### 4단계: 웹마스터도구 등록

- **사이트 등록**: 웹마스터도구에 신규 사이트 등록
- **사이트맵 제출**: sitemap.xml 제출
- **소유확인**: 사이트 소유확인 후 수집/색인 현황 확인
- **품질평가**: 시간 경과에 따라 구 사이트 수준까지 품질평가 상승

#### 5단계: 구 사이트 종료

- **수집 요청**: 구 사이트 콘텐츠가 노출 중이면 웹페이지 수집 요청 사용
- **서버 종료**: 사용자 유입이 없으면 서버 종료
- **사이트 삭제**: 웹마스터도구에서 구 사이트 삭제

### 주의사항

#### 절대 금지사항

- **robots.txt 먼저 설정**: 이전 완료 전까지 구 사이트 robots.txt 설정 금지
- **네트워크 정지**: 서버 네트워크 먼저 정지하거나 블록 금지
- **접근 차단**: 검색로봇의 확인 접근 차단 금지

#### 문제 발생 원인

- **확인 불가**: robots.txt 설정으로 로봇의 사이트 변경 확인 접근 차단
- **상태 파악 불가**: 네트워크 정지로 로봇이 서버 상태 파악 불가
- **반영 실패**: 원하는 동작 반영 불가능

#### 권장사항

- **주요 페이지 확인**: 모든 주요 페이지 변경/미노출 확인 후 robots.txt 설정
- **단계적 접근**: 갑작스러운 차단보다는 단계적 접근 권장

### AI 작업 체크리스트

#### 이전 준비

- [ ] 콘텐츠 규모 파악 (중소/대규모)
- [ ] 이전 전략 수립 (전체/단계적)
- [ ] 도메인 변경 필요성 재검토

#### 이전 실행

- [ ] 전체 콘텐츠 및 파일 이동
- [ ] 신규 도메인 할당 및 DNS 설정
- [ ] 301 HTTP redirect 설정
- [ ] canonical URL 설정
- [ ] robots.txt 허용 설정

#### 웹마스터도구 관리

- [ ] 신규 사이트 등록
- [ ] 사이트맵 제출
- [ ] 소유확인 완료
- [ ] 수집/색인 현황 모니터링

#### 구 사이트 정리

- [ ] 주요 페이지 변경 확인
- [ ] 웹페이지 수집 요청 (필요시)
- [ ] 서버 종료 및 사이트 삭제

### 코드 템플릿

#### Next.js 리다이렉트 설정

```typescript
// next.config.ts
async redirects() {
  return [
    // 구 사이트에서 신규 사이트로 301 리다이렉트
    {
      source: '/:path*',
      has: [{ type: 'host', value: 'old-domain.com' }],
      destination: 'https://new-domain.com/:path*',
      permanent: true,
    }
  ];
}
```

#### canonical URL 설정

```html
<!-- 각 페이지에 canonical URL 설정 -->
<link rel="canonical" href="https://new-domain.com/current-page" />
```

#### robots.txt 설정

```txt
# 신규 사이트 robots.txt (모두 허용)
User-agent: *
Allow: /

# 사이트맵 위치
Sitemap: https://new-domain.com/sitemap.xml
```

이 웹사이트 이전 가이드를 모든 사이트 이전 작업에 체계적으로 적용하여 네이버 검색 최적화를 완성할 것.

---

## rule6 웹 사이트 운영을 그만둘 때

> 출처: [네이버 검색 어드바이저 웹사이트 운영 종료 가이드](https://searchadvisor.naver.com/guide/seo-basic-close)

### 깔끔한 사이트 종료 지침

#### 브랜드 보호 및 사용자 배려

##### 운영 정지 시 조치사항

- **사용자 권한 회수**: 관리자를 제외한 모든 사용자의 쓰기 권한 회수
- **브랜드 기억**: 방문 사용자들이 사이트 이름과 브랜드를 기억하고 있음
- **나쁜 기억 방지**: 브랜드가 나쁜 기억이 되지 않도록 주의

##### 서비스 종료 시 조치사항

- **사전 공지**: 최소 1개월 이상 사이트 종료 공지
- **사용자 안내**: 서비스 종료 이유와 대안 정보 제공
- **단계적 종료**: 갑작스러운 종료보다는 점진적 종료 권장

### 사이트 방치 시 문제점

#### 스팸 및 저품질 콘텐츠 문제

##### 게시판 스팸화

- **관리 포기**: 관리를 포기한 사이트의 게시판은 스팸으로 가득 차기 쉬움
- **콘텐츠 품질 저하**: 유용한 정보가 스팸으로 대체됨
- **사용자 경험 악화**: 기존 사용자들의 부정적 경험

#### 도메인 만기 문제

##### 도메인 만기 시 위험

- **자동 연결**: 도메인이 만기되어도 웹사이트 연결이 끊기지 않음
- **도메인 판매업자**: 만기된 도메인은 판매업자에게 넘어감
- **저품질 페이지 연결**: 기존 사용자들이 성인, 불법, p2p 사이트, 쇼핑몰 등으로 연결될 위험

##### 직접 변경 위험

- **불법 사이트**: 만기된 도메인이 불법, 음란, 도박, 스팸 사이트로 직접 변경
- **브랜드 손상**: 기존 브랜드와 무관한 저품질 콘텐츠로 연결
- **사용자 혼란**: 기존 사용자들이 예상치 못한 콘텐츠에 노출

### 사이트 종료 절차

#### 사전 준비 단계

##### 1단계: 종료 계획 수립

- [ ] 종료 일정 결정 (최소 1개월 전 공지)
- [ ] 사용자 안내 방법 결정
- [ ] 대체 서비스 또는 대안 정보 준비
- [ ] 데이터 백업 및 보관 계획

##### 2단계: 사용자 공지

- [ ] 사이트 메인페이지에 종료 공지 게시
- [ ] 이메일 뉴스레터 발송 (가입자 대상)
- [ ] 소셜미디어 채널을 통한 공지
- [ ] 주요 페이지에 종료 안내 배너

##### 3단계: 권한 제한

- [ ] 일반 사용자 쓰기 권한 회수
- [ ] 댓글 및 게시글 작성 제한
- [ ] 회원가입 기능 중단
- [ ] 관리자만 접근 가능하도록 설정

#### 종료 실행 단계

##### 4단계: 서비스 중단

- [ ] 새로운 콘텐츠 업로드 중단
- [ ] 사용자 상호작용 기능 비활성화
- [ ] 결제 및 주문 기능 중단 (해당 시)
- [ ] API 서비스 중단 (해당 시)

##### 5단계: 도메인 관리

- [ ] 도메인 갱신 중단 계획 수립
- [ ] 도메인 만기일 확인
- [ ] 도메인 판매업자 정보 확인
- [ ] 대체 도메인 준비 (필요시)

##### 6단계: 최종 정리

- [ ] 중요 데이터 백업 완료
- [ ] 사용자 개인정보 삭제
- [ ] 서버 정리 및 비용 절약
- [ ] 웹마스터도구에서 사이트 삭제

### AI 작업 체크리스트

#### 사전 준비

- [ ] 종료 일정 및 계획 수립
- [ ] 사용자 안내 방법 결정
- [ ] 대체 서비스 정보 준비
- [ ] 데이터 백업 계획 수립

#### 사용자 공지

- [ ] 사이트 메인페이지 공지 게시
- [ ] 이메일 뉴스레터 발송
- [ ] 소셜미디어 공지
- [ ] 주요 페이지 배너 설치

#### 권한 제한

- [ ] 일반 사용자 쓰기 권한 회수
- [ ] 댓글/게시글 작성 제한
- [ ] 회원가입 기능 중단
- [ ] 관리자 전용 접근 설정

#### 서비스 중단

- [ ] 새 콘텐츠 업로드 중단
- [ ] 사용자 상호작용 비활성화
- [ ] 결제/주문 기능 중단
- [ ] API 서비스 중단

#### 도메인 관리

- [ ] 도메인 갱신 중단 계획
- [ ] 도메인 만기일 확인
- [ ] 도메인 판매업자 정보 확인
- [ ] 대체 도메인 준비

#### 최종 정리

- [ ] 중요 데이터 백업 완료
- [ ] 사용자 개인정보 삭제
- [ ] 서버 정리 및 비용 절약
- [ ] 웹마스터도구 사이트 삭제

### 절대 금지사항

#### 갑작스러운 종료

1. **사전 공지 없이 갑작스러운 종료**
2. **사용자 안내 없이 서비스 중단**
3. **데이터 백업 없이 서버 삭제**

#### 도메인 방치

4. **도메인 만기 후 방치**
5. **도메인 갱신 중단으로 인한 저품질 사이트 연결**
6. **브랜드 보호 조치 미실행**

#### 사용자 배려 부족

7. **사용자 권한 회수 없이 방치**
8. **스팸 게시판 방치**
9. **개인정보 보호 조치 미실행**

### 검증 방법

#### 사전 검증

1. **종료 계획 완성도 확인**: 모든 단계별 계획 수립
2. **사용자 안내 방법 검증**: 다양한 채널을 통한 공지 방법
3. **데이터 백업 확인**: 중요 데이터 백업 완료 여부
4. **도메인 관리 계획**: 도메인 만기일 및 갱신 계획

#### 실행 중 검증

1. **사용자 반응 모니터링**: 공지 후 사용자 반응 확인
2. **기능 중단 테스트**: 각 기능별 중단 상태 확인
3. **권한 제한 확인**: 일반 사용자 접근 제한 상태 확인
4. **데이터 보호 확인**: 개인정보 삭제 및 보호 조치

#### 최종 검증

1. **서버 정리 완료**: 불필요한 서버 리소스 정리
2. **도메인 상태 확인**: 도메인 만기 및 연결 상태
3. **웹마스터도구 정리**: 사이트 등록 삭제 완료
4. **비용 절약 확인**: 서버 비용 및 도메인 비용 절약

### 종료 공지 템플릿

#### 사이트 메인페이지 공지

```html
<!-- 사이트 종료 공지 -->
<div class="notice-banner">
  <h2>사이트 종료 안내</h2>
  <p>안녕하세요. {사이트명}을 이용해 주시는 고객님들께 안내드립니다.</p>
  <p><strong>종료 일정</strong>: {종료일자}</p>
  <p><strong>종료 사유</strong>: {종료 사유}</p>
  <p><strong>대안 정보</strong>: {대체 서비스 또는 대안 정보}</p>
  <p>그동안 이용해 주셔서 감사합니다.</p>
</div>
```

#### 이메일 뉴스레터

```html
<!-- 이메일 공지 템플릿 -->
<h1>{사이트명} 서비스 종료 안내</h1>
<p>안녕하세요, {사이트명}을 이용해 주시는 고객님들께 안내드립니다.</p>
<p>서비스 종료 일정: {종료일자}</p>
<p>종료 사유: {종료 사유}</p>
<p>대안 정보: {대체 서비스 정보}</p>
<p>문의사항: {연락처}</p>
<p>그동안 이용해 주셔서 감사합니다.</p>
```

이 웹사이트 운영 종료 가이드를 모든 사이트 종료 작업에 체계적으로 적용하여 브랜드 보호와 사용자 배려를 완성할 것.

---

## rule7 검색로봇 확인 방법

> 출처: [네이버 검색 어드바이저 검색로봇 확인 방법](https://searchadvisor.naver.com/guide/seo-basic-firewall)

### User-Agent 기반 검색로봇 식별

#### 네이버 검색로봇 User-Agent

##### Yeti 검색로봇

- **주요 User-Agent**: `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko; compatible; Yeti/1.1; +https://naver.me/spd) Chrome/W.X.Y.Z Safari/537.36`
- **간단 버전**: `Mozilla/5.0 (compatible; Yeti/1.1; +https://naver.me/spd)`
- **용도**: 일반적인 네이버 검색 서비스를 위한 웹 문서 수집
- **특징**: 세부 버전은 사전 예고 없이 변경될 수 있음

##### 전문 로봇들

- **Ads-Naver**: 광고주 페이지에서 효과적인 광고 노출을 위한 추가 정보 수집
- **Blueno**: 블로그 에디터 등에서 링크 삽입 시 미리보기 수집

### IP 기반 검색로봇 식별

#### 역 DNS 조회 방법

##### 리눅스/MacOS 명령어

```bash
# 1단계: IP 주소로 역 DNS 조회
$ host 125.209.235.169
169.235.209.125.in-addr.arpa domain name pointer crawl.125-209-235-169.web.naver.com.

# 2단계: 도메인으로 DNS 조회
$ host crawl.125-209-235-169.web.naver.com
crawl.125-209-235-169.web.naver.com has address 125.209.235.169
```

##### Windows 명령어

```cmd
# 1단계: IP 주소로 역 DNS 조회
C:\Users> nslookup 125.209.235.169
Server: 168.126.63.1
Address: 168.126.63.1

Name: crawl.125-209-235-169.web.naver.com
Address: 125.209.235.169

# 2단계: 도메인으로 DNS 조회
C:\Users> nslookup crawl.125-209-235-169.web.naver.com
Server: 168.126.63.1
Address: 168.126.63.1

Name: crawl.125-209-235-169.web.naver.com
Address: 125.209.235.169
```

#### 확인 기준

1. **도메인 확인**: 조회된 도메인이 `.naver.com`으로 끝나는지 확인
2. **IP 일치 확인**: DNS 조회 결과가 원래 IP 주소와 일치하는지 확인

### 방화벽 설정 점검

#### 검색 노출 문제 진단

##### 사이트가 검색에 노출되지 않는 경우

- **User-Agent 차단 확인**: 방화벽에서 `Yeti` User-Agent 차단 여부 확인
- **IP 주소 차단 확인**: 네이버 서버 IP 범위 차단 여부 확인
- **접근 로그 분석**: 웹서버 로그에서 네이버 로봇 접근 기록 확인

##### 정보가 변경되지 않는 경우

- **캐시 문제**: 검색 결과 캐시로 인한 지연 가능성
- **크롤링 빈도**: 네이버 로봇의 재방문 주기 확인
- **서버 응답**: 웹서버가 정상적으로 응답하는지 확인

### AI 작업 체크리스트

#### User-Agent 확인

- [ ] 네이버 Yeti 로봇 User-Agent 패턴 확인
- [ ] 방화벽에서 Yeti User-Agent 차단 여부 점검
- [ ] 웹서버 로그에서 Yeti 접근 기록 확인
- [ ] 전문 로봇 (Ads-Naver, Blueno) 접근 허용 설정

#### IP 기반 확인

- [ ] 접근 로그에서 의심스러운 IP 주소 식별
- [ ] 역 DNS 조회로 `.naver.com` 도메인 확인
- [ ] DNS 조회로 IP 주소 일치성 확인
- [ ] 네이버 IP 범위 목록과 대조 검증

#### 방화벽 설정 점검

- [ ] 방화벽에서 네이버 서버 IP 차단 여부 확인
- [ ] User-Agent 기반 차단 규칙 점검
- [ ] 웹서버 접근 제한 설정 확인
- [ ] 로봇 접근 허용 정책 검토

#### 로그 분석

- [ ] 웹서버 접근 로그 분석
- [ ] 방화벽 접근 기록 확인
- [ ] 네이버 로봇 접근 패턴 분석
- [ ] 차단된 요청 패턴 식별

### 검증 방법

#### User-Agent 검증

1. **로그 분석**: 웹서버 로그에서 `Yeti` User-Agent 확인
2. **방화벽 설정**: 방화벽에서 `Yeti` 차단 규칙 제거
3. **접근 테스트**: 네이버 로봇 시뮬레이션 접근 테스트
4. **응답 확인**: 정상적인 HTTP 응답 코드 반환 확인

#### IP 검증

1. **역 DNS 조회**: 의심스러운 IP에 대한 역 DNS 조회 실행
2. **도메인 확인**: `.naver.com` 도메인 확인
3. **IP 일치 확인**: DNS 조회 결과와 원래 IP 일치 확인
4. **목록 대조**: 네이버 IP 범위 목록과 대조

#### 방화벽 검증

1. **설정 점검**: 방화벽 설정에서 네이버 IP 차단 여부 확인
2. **규칙 테스트**: User-Agent 기반 차단 규칙 테스트
3. **접근 허용**: 네이버 로봇 접근 허용 규칙 추가
4. **모니터링**: 로봇 접근 패턴 모니터링

### 문제 해결 가이드

#### 검색 노출 문제 해결

1. **로그 확인**: 웹서버 로그에서 네이버 로봇 접근 확인
2. **방화벽 점검**: 방화벽에서 네이버 로봇 차단 여부 확인
3. **설정 수정**: 차단된 경우 접근 허용 설정 추가
4. **재검증**: 수정 후 네이버 로봇 접근 재확인

#### 정보 업데이트 지연 해결

1. **캐시 확인**: 검색 결과 캐시로 인한 지연 가능성 확인
2. **크롤링 주기**: 네이버 로봇 재방문 주기 확인
3. **서버 응답**: 웹서버 정상 응답 확인
4. **수동 요청**: 네이버 웹마스터도구에서 수동 크롤링 요청

이 검색로봇 확인 방법을 모든 웹사이트 운영에 체계적으로 적용하여 네이버 검색 최적화를 완성할 것.

---

## rule8 HTTP 규약(Protocol)

> 출처: [네이버 검색 어드바이저 HTTP 규약 가이드](https://searchadvisor.naver.com/guide/seo-basic-http)

### HTTP 규약 개요

#### HTTP 규약 정의

- **브라우저 통신**: Browser (IE, Chrome, Safari, Firefox)가 web server와 통신하기 위한 규약
- **요청-응답 구조**: Browser에서 요청(Request)하면 응답(Response)하는 간단한 구조
- **국제 표준**: W3C, IETF 등에서 제정한 인터넷 표준 (RFC-2616)
- **안정성**: 1991년 HTTP/0.9부터 20년 이상 전세계적으로 사용되는 안정된 표준
- **적용 범위**: WWW와 관련된 모든 software가 따르고 있음

### HTTP 응답코드 분류

#### 1xx - Informational (정보)

- **용도**: 요청 처리 중임을 알리는 임시 응답
- **특징**: 실제 응답 전에 전송되는 상태 정보

#### 2xx - Successful (성공)

- **200 OK**: 가장 일반적인 경우, 요청된 웹 페이지를 돌려줄 경우
- **용도**: 정상적인 요청 처리 완료
- **SEO 영향**: 검색엔진이 정상적으로 콘텐츠를 수집할 수 있음

#### 3xx - Redirection (리다이렉션)

- **301 Moved Permanently**: 요청된 URL이 완전히 전환된 경우, client는 요청된 URL을 지우거나 새 URL로 교체
- **302 Found**: 임시로 변경된 것을 나타내지만, 실제 구현이 HTTP 규약 의도를 벗어나서 303과 307로 분리
- **303 See Other**: 요청된 URL이 잠시 다른 URL로 바뀐 것을 알림, GET method로 접근해야 함
- **307 Temporary Redirect**: 요청된 URL이 잠시 다른 URL로 바뀐 것을 알림, GET method로 접근해야 함
- **308 Permanent Redirect**: 요청된 URL이 완전히 전환된 경우, 요청 방식(POST, PUT, DELETE 등)이 유지됨

#### 4xx - Client Error (클라이언트 오류)

- **400 Bad Request**: HTTP 요청, 특히 문법이 잘못된 경우
- **401 Unauthorized**: 웹 페이지 접근 시 필요한 인증 자격이 없거나 부족한 경우
- **403 Forbidden**: 인증 정보는 있지만 권한이 없는 웹 페이지에 접근했을 경우
- **404 Not Found**: 존재하지 않는 페이지에 접근했을 경우

#### 5xx - Server Error (서버 오류)

- **500 Internal Server Error**: 웹 서버 설정이 잘못 되었거나 서버 프로그램에 오류가 있을 때
- **503 Service Unavailable**: 웹 서버에 너무 많은 요청이 몰리거나 웹 서버에 부하가 걸려 응답하지 못할 때

### SEO 관점에서의 HTTP 응답코드

#### 검색엔진 최적화에 유리한 응답코드

##### 200 OK (최적)

- **검색엔진 영향**: 정상적인 콘텐츠 수집 가능
- **사용자 경험**: 빠른 페이지 로딩
- **SEO 점수**: 높은 SEO 점수 획득

##### 301 Moved Permanently (양호)

- **검색엔진 영향**: 페이지 권한이 새 URL로 전달됨
- **사용자 경험**: 자동으로 새 페이지로 이동
- **SEO 점수**: 중간 SEO 점수, 권한 전달로 인한 점수 유지

#### 검색엔진 최적화에 불리한 응답코드

##### 404 Not Found (불량)

- **검색엔진 영향**: 콘텐츠 수집 불가능
- **사용자 경험**: 페이지를 찾을 수 없음
- **SEO 점수**: 낮은 SEO 점수, 검색 결과에서 제외 가능

##### 500 Internal Server Error (매우 불량)

- **검색엔진 영향**: 서버 오류로 인한 수집 실패
- **사용자 경험**: 서버 오류 메시지
- **SEO 점수**: 매우 낮은 SEO 점수, 검색 결과에서 제외

##### 503 Service Unavailable (불량)

- **검색엔진 영향**: 일시적 서버 부하로 인한 수집 지연
- **사용자 경험**: 서비스 일시 중단 메시지
- **SEO 점수**: 낮은 SEO 점수, 일시적 검색 결과 제외

### AI 작업 체크리스트

#### 서버 응답 상태 모니터링

- [ ] 웹서버 응답코드 정기 점검
- [ ] 200 OK 응답 비율 모니터링
- [ ] 4xx, 5xx 오류 발생률 추적
- [ ] 응답 시간 모니터링

#### 리다이렉션 관리

- [ ] 301 리다이렉션 설정 확인
- [ ] 302 리다이렉션 사용 최소화
- [ ] 리다이렉션 체인 점검
- [ ] 리다이렉션 성능 최적화

#### 오류 페이지 관리

- [ ] 404 오류 페이지 커스터마이징
- [ ] 500 오류 페이지 개선
- [ ] 503 오류 시 대체 페이지 제공
- [ ] 오류 로그 분석 및 대응

#### 성능 최적화

- [ ] 서버 응답 시간 최적화
- [ ] 캐싱 설정 개선
- [ ] CDN 활용 검토
- [ ] 서버 부하 분산

### 검증 방법

#### 응답코드 검증

1. **정상 응답 확인**: 주요 페이지의 200 OK 응답 확인
2. **리다이렉션 검증**: 301 리다이렉션 정상 작동 확인
3. **오류 페이지 점검**: 404, 500 오류 페이지 적절성 확인
4. **성능 테스트**: 응답 시간 및 서버 부하 테스트

#### SEO 영향 분석

1. **검색 결과 확인**: 주요 페이지의 검색 노출 상태 확인
2. **크롤링 로그 분석**: 검색엔진 크롤링 패턴 분석
3. **사용자 경험 평가**: 페이지 로딩 속도 및 접근성 평가
4. **웹마스터도구 활용**: Google Search Console, Naver Webmaster Tools 활용

### 문제 해결 가이드

#### 404 오류 해결

1. **링크 점검**: 깨진 링크 식별 및 수정
2. **리다이렉션 설정**: 301 리다이렉션으로 올바른 페이지로 연결
3. **사이트맵 업데이트**: XML 사이트맵에서 오류 페이지 제거
4. **검색엔진 알림**: 웹마스터도구에서 404 오류 제거 요청

#### 500 오류 해결

1. **서버 로그 분석**: 오류 원인 식별
2. **코드 점검**: 서버 코드 오류 수정
3. **설정 확인**: 웹서버 설정 점검
4. **백업 복구**: 필요시 백업에서 복구

#### 503 오류 해결

1. **서버 부하 분석**: 트래픽 증가 원인 파악
2. **리소스 확장**: 서버 리소스 증설
3. **캐싱 강화**: CDN 및 캐싱 설정 개선
4. **부하 분산**: 로드 밸런서 도입 검토

이 HTTP 규약 가이드를 모든 웹사이트 운영에 체계적으로 적용하여 검색엔진 최적화를 완성할 것.

---
